{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use H97_1__0_1_2 vs:\n",
    "\n",
    "- H97_1__0_1\n",
    "\n",
    "- H97_1__0_2\n",
    "\n",
    "- H97_1__1_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data train 3 model: \n",
    "\n",
    "- H97_1__0_1\n",
    "\n",
    "- H97_1__0_2\n",
    "\n",
    "- H97_1__1_2\n",
    "\n",
    "Form:\n",
    "```markdown\n",
    "train\n",
    "    |___B2\n",
    "        |___record1: [xs1, xs2, xs3]\n",
    "        |___record2: [xs1, xs2, xs3]\n",
    "    |___B5\n",
    "    |___B6\n",
    "valid\n",
    "    |___B2\n",
    "    |___B5\n",
    "    |___B6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download code and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Harito97/AI_Product_ThyroidCancerClassifier.git\n",
    "%cd AI_Product_ThyroidCancerClassifier\n",
    "!git pull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data train model:\n",
    "\n",
    "```markdown\n",
    "train\n",
    "    |___B2: [[xs1, xs2, xs3], [xs1, xs2, xs3], ...]\n",
    "    |___B5: [[xs1, xs2, xs3], [xs1, xs2, xs3], ...]\n",
    "    |___B6: [[xs1, xs2, xs3], [xs1, xs2, xs3], ...]\n",
    "valid\n",
    "    |___B2: [[xs1, xs2, xs3], [xs1, xs2, xs3], ...]\n",
    "    |___B5: [[xs1, xs2, xs3], [xs1, xs2, xs3], ...]\n",
    "    |___B6: [[xs1, xs2, xs3], [xs1, xs2, xs3], ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataver1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def count_images_per_class(folder_path):\n",
    "    image_counts = {}\n",
    "    for subfolder in ['B2', 'B5', 'B6']:\n",
    "        folder = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(folder):\n",
    "            num_images = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "            image_counts[subfolder] = num_images\n",
    "    return image_counts\n",
    "\n",
    "# Đường dẫn đến thư mục dữ liệu\n",
    "train_folder = '/dataver1/train'\n",
    "valid_folder = '/dataver1/valid'\n",
    "\n",
    "# Đếm số ảnh ở mỗi nhãn\n",
    "train_counts = count_images_per_class(train_folder)\n",
    "valid_counts = count_images_per_class(valid_folder)\n",
    "\n",
    "# Chuyển đổi dữ liệu thành DataFrame và in kết quả\n",
    "df_train = pd.DataFrame(list(train_counts.items()), columns=['Class', 'Number of Images'])\n",
    "df_valid = pd.DataFrame(list(valid_counts.items()), columns=['Class', 'Number of Images'])\n",
    "\n",
    "print(\"Train Data\")\n",
    "print(df_train)\n",
    "print(\"Validation Data\")\n",
    "print(df_valid)\n",
    "\n",
    "# Weight of each class\n",
    "import numpy as np\n",
    "\n",
    "# Tính trọng số cho hàm loss\n",
    "total_train_images = sum(train_counts.values())\n",
    "class_weights = {\n",
    "    class_name: total_train_images / (len(train_counts) * num_images)\n",
    "    for class_name, num_images in train_counts.items()\n",
    "}\n",
    "\n",
    "# In trọng số\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for label, subfolder in enumerate([\"B2\", \"B5\", \"B6\"]):\n",
    "            folder = os.path.join(folder_path, subfolder)\n",
    "            for img_name in os.listdir(folder):\n",
    "                if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_paths.append(os.path.join(folder, img_name))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Tăng cường dữ liệu\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_valid = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train_dataset = CustomDataset(train_folder, transform=transform_train)\n",
    "train_dataset = CustomDataset(train_folder, transform=transform_valid)\n",
    "valid_dataset = CustomDataset(valid_folder, transform=transform_valid)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model H97_1__0_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /path/to/AI_Product_ThyroidCancerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.H97_1__0_1_2 import H97_1__0_1_2\n",
    "\n",
    "h97_1__0_1_2 = H97_1__0_1_2()\n",
    "h97_1__0_1_2.load_state_dict(\n",
    "    torch.load(\"output_ex/best_model_CNN_B2_B5_B6_dataver1_trainx1_30epoch.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, auc, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, valid_loader):\n",
    "    model.load_state_dict(torch.load('output_ex/best_model_CNN_B2_B5_B6_dataver1_trainx1_30epoch.pth'))\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_preds_prob = [], [], []\n",
    "    \n",
    "    output_of_H97_1__0_1_2 = {x: [], y: []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "\n",
    "            output_of_H97_1__0_1_2[x].append(outputs)\n",
    "            output_of_H97_1__0_1_2[y].append(labels)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            preds_prob = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds_prob.extend(preds_prob)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Log metrics to WandB\n",
    "    wandb.log({'accuracy': accuracy, 'f1_score': f1})\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['B2', 'B5', 'B6'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC AUC\n",
    "    all_labels_bin = label_binarize(all_labels, classes=[0, 1, 2])\n",
    "    all_preds_prob = np.array(all_preds_prob)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(all_labels_bin.shape[1]):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_bin[:, i], all_preds_prob[:, i])\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (area = {auc(fpr, tpr):.2f})')\n",
    "    \n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    return output_of_H97_1__0_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"AI_Product_ThyroidCancerClassifier\", name='CNN_B2_B5_B6_dataver1_test_on_train')\n",
    "output_of_H97_1__0_1_2_on_trainset = evaluate_model(model, train_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"AI_Product_ThyroidCancerClassifier\", name='CNN_B2_B5_B6_dataver1_test_on_train')\n",
    "output_of_H97_1__0_1_2_on_validset = evaluate_model(model, valid_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output of H97_1__0_1_2 on train set:', output_of_H97_1__0_1_2_on_trainset)\n",
    "print('Output of H97_1__0_1_2 on valid set:', output_of_H97_1__0_1_2_on_validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train set output\n",
    "with open('output_ex/output_of_H97_1__0_1_2_on_trainset.pkl', 'wb') as f:\n",
    "    pickle.dump(output_of_H97_1__0_1_2_on_trainset, f)\n",
    "\n",
    "# Save the validation set output\n",
    "with open('output_ex/output_of_H97_1__0_1_2_on_validset.pkl', 'wb') as f:\n",
    "    pickle.dump(output_of_H97_1__0_1_2_on_validset, f)\n",
    "\n",
    "# To read the saved outputs later\n",
    "with open('output_ex/output_of_H97_1__0_1_2_on_trainset.pkl', 'rb') as f:\n",
    "    loaded_trainset_output = pickle.load(f)\n",
    "\n",
    "with open('output_ex/output_of_H97_1__0_1_2_on_validset.pkl', 'rb') as f:\n",
    "    loaded_validset_output = pickle.load(f)\n",
    "\n",
    "# Print to verify\n",
    "print(loaded_trainset_output)\n",
    "print(loaded_validset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train H97_1__i_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train H97_1__0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def filter_data(output_dict, labels_to_include):\n",
    "    filtered_outputs = {x: [], y: []}\n",
    "    for outputs, labels in zip(output_dict[x], output_dict[y]):\n",
    "        mask = np.isin(labels.cpu().numpy(), labels_to_include)\n",
    "        filtered_outputs[x].append(outputs[mask])\n",
    "        filtered_outputs[y].append(labels[mask])\n",
    "    return filtered_outputs\n",
    "\n",
    "# Filter to include only labels 0 and 1\n",
    "labels_to_include = [0, 1]\n",
    "filtered_trainset_output = filter_data(output_of_H97_1__0_1_2_on_trainset, labels_to_include)\n",
    "filtered_validset_output = filter_data(output_of_H97_1__0_1_2_on_validset, labels_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def prepare_dataloader(filtered_output, batch_size=32):\n",
    "    all_outputs = torch.cat(filtered_output[x])\n",
    "    all_labels = torch.cat(filtered_output[y])\n",
    "    dataset = TensorDataset(all_outputs, all_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_loader_filtered = prepare_dataloader(filtered_trainset_output)\n",
    "valid_loader_filtered = prepare_dataloader(filtered_validset_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "# Function to extract labels from DataLoader\n",
    "def extract_labels_from_dataloader(dataloader):\n",
    "    all_labels = []\n",
    "    for _, labels in dataloader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_labels)\n",
    "\n",
    "# Extract labels from train_loader_filtered\n",
    "train_labels = extract_labels_from_dataloader(train_loader_filtered)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "class_weights = class_weights_dict\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "# class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Khởi tạo WandB\n",
    "wandb.init(\n",
    "    project=\"AI_Product_ThyroidCancerClassifier\",\n",
    "    name=\"h97_1__0_1_dataver1\",\n",
    ")\n",
    "\n",
    "\n",
    "def train(model, train_loader, valid_loader, num_epochs=100, patience=15):\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).cuda()\n",
    "    )\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_valid_loss < best_loss:\n",
    "            best_loss = epoch_valid_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                \"output_ex/best_model_h97_1__0_1_dataver1.pth\",\n",
    "            )\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Tính toán các chỉ số\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"f1_score\": f1,\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"valid_loss\": epoch_valid_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# Train model\n",
    "from model.H97_1__i_j import H97_1__i_j\n",
    "\n",
    "h97_1__0_1 = H97_1__i_j()\n",
    "h97_1__0_1.cuda()\n",
    "train(h97_1__0_1, train_loader_filtered, valid_loader_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, auc, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, valid_loader):\n",
    "    model.load_state_dict(torch.load('output_ex/best_model_h97_1__0_1_dataver1.pth'))\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_preds_prob = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            preds_prob = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds_prob.extend(preds_prob)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Log metrics to WandB\n",
    "    wandb.log({'accuracy': accuracy, 'f1_score': f1})\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['B2', 'B5', 'B6'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC AUC\n",
    "    all_labels_bin = label_binarize(all_labels, classes=[0, 1, 2])\n",
    "    all_preds_prob = np.array(all_preds_prob)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(all_labels_bin.shape[1]):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_bin[:, i], all_preds_prob[:, i])\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (area = {auc(fpr, tpr):.2f})')\n",
    "    \n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "wandb.init(project=\"AI_Product_ThyroidCancerClassifier\", name='h97_1__0_1_dataver1_test')\n",
    "evaluate_model(h97_1__0_1, valid_loader_filtered)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train H97_1__1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def filter_data(output_dict, labels_to_include):\n",
    "    filtered_outputs = {x: [], y: []}\n",
    "    for outputs, labels in zip(output_dict[x], output_dict[y]):\n",
    "        mask = np.isin(labels.cpu().numpy(), labels_to_include)\n",
    "        filtered_outputs[x].append(outputs[mask])\n",
    "        filtered_outputs[y].append(labels[mask])\n",
    "    return filtered_outputs\n",
    "\n",
    "# Filter to include only labels 1 and 2\n",
    "labels_to_include = [1, 2]\n",
    "filtered_trainset_output = filter_data(output_of_H97_1__0_1_2_on_trainset, labels_to_include)\n",
    "filtered_validset_output = filter_data(output_of_H97_1__0_1_2_on_validset, labels_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def prepare_dataloader(filtered_output, batch_size=32):\n",
    "    all_outputs = torch.cat(filtered_output[x])\n",
    "    all_labels = torch.cat(filtered_output[y])\n",
    "    dataset = TensorDataset(all_outputs, all_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_loader_filtered = prepare_dataloader(filtered_trainset_output)\n",
    "valid_loader_filtered = prepare_dataloader(filtered_validset_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "# Function to extract labels from DataLoader\n",
    "def extract_labels_from_dataloader(dataloader):\n",
    "    all_labels = []\n",
    "    for _, labels in dataloader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_labels)\n",
    "\n",
    "# Extract labels from train_loader_filtered\n",
    "train_labels = extract_labels_from_dataloader(train_loader_filtered)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "class_weights = class_weights_dict\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "# class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Khởi tạo WandB\n",
    "wandb.init(\n",
    "    project=\"AI_Product_ThyroidCancerClassifier\",\n",
    "    name=\"h97_1__1_2_dataver1\",\n",
    ")\n",
    "\n",
    "\n",
    "def train(model, train_loader, valid_loader, num_epochs=100, patience=15):\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).cuda()\n",
    "    )\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_valid_loss < best_loss:\n",
    "            best_loss = epoch_valid_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                \"output_ex/best_model_h97_1__1_2_dataver1.pth\",\n",
    "            )\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Tính toán các chỉ số\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"f1_score\": f1,\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"valid_loss\": epoch_valid_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# Train model\n",
    "from model.H97_1__i_j import H97_1__i_j\n",
    "\n",
    "h97_1__1_2 = H97_1__i_j()\n",
    "h97_1__1_2.cuda()\n",
    "train(h97_1__1_2, train_loader_filtered, valid_loader_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train H97_1__0_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def filter_data(output_dict, labels_to_include):\n",
    "    filtered_outputs = {x: [], y: []}\n",
    "    for outputs, labels in zip(output_dict[x], output_dict[y]):\n",
    "        mask = np.isin(labels.cpu().numpy(), labels_to_include)\n",
    "        filtered_outputs[x].append(outputs[mask])\n",
    "        filtered_outputs[y].append(labels[mask])\n",
    "    return filtered_outputs\n",
    "\n",
    "# Filter to include only labels 0 and 2\n",
    "labels_to_include = [0, 2]\n",
    "filtered_trainset_output = filter_data(output_of_H97_1__0_1_2_on_trainset, labels_to_include)\n",
    "filtered_validset_output = filter_data(output_of_H97_1__0_1_2_on_validset, labels_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def prepare_dataloader(filtered_output, batch_size=32):\n",
    "    all_outputs = torch.cat(filtered_output[x])\n",
    "    all_labels = torch.cat(filtered_output[y])\n",
    "    dataset = TensorDataset(all_outputs, all_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_loader_filtered = prepare_dataloader(filtered_trainset_output)\n",
    "valid_loader_filtered = prepare_dataloader(filtered_validset_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "# Function to extract labels from DataLoader\n",
    "def extract_labels_from_dataloader(dataloader):\n",
    "    all_labels = []\n",
    "    for _, labels in dataloader:\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_labels)\n",
    "\n",
    "# Extract labels from train_loader_filtered\n",
    "train_labels = extract_labels_from_dataloader(train_loader_filtered)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "class_weights = class_weights_dict\n",
    "# Convert class weights to a tensor\n",
    "# class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Khởi tạo WandB\n",
    "wandb.init(\n",
    "    project=\"AI_Product_ThyroidCancerClassifier\",\n",
    "    name=\"h97_1__0_2_dataver1\",\n",
    ")\n",
    "\n",
    "\n",
    "def train(model, train_loader, valid_loader, num_epochs=100, patience=15):\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).cuda()\n",
    "    )\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_valid_loss < best_loss:\n",
    "            best_loss = epoch_valid_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                \"output_ex/best_model_h97_1__0_2_dataver1.pth\",\n",
    "            )\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Tính toán các chỉ số\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"f1_score\": f1,\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"valid_loss\": epoch_valid_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# Train model\n",
    "from model.H97_1__i_j import H97_1__i_j\n",
    "\n",
    "h97_1__0_2 = H97_1__i_j()\n",
    "h97_1__0_2.cuda()\n",
    "train(h97_1__0_2, train_loader_filtered, valid_loader_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, auc, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, valid_loader):\n",
    "    model.load_state_dict(torch.load('output_ex/best_model_h97_1__0_2_dataver1.pth'))\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_preds_prob = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            preds_prob = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds_prob.extend(preds_prob)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Log metrics to WandB\n",
    "    wandb.log({'accuracy': accuracy, 'f1_score': f1})\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['B2', 'B5', 'B6'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC AUC\n",
    "    all_labels_bin = label_binarize(all_labels, classes=[0, 1, 2])\n",
    "    all_preds_prob = np.array(all_preds_prob)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(all_labels_bin.shape[1]):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_bin[:, i], all_preds_prob[:, i])\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (area = {auc(fpr, tpr):.2f})')\n",
    "    \n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "wandb.init(project=\"AI_Product_ThyroidCancerClassifier\", name='h97_1__0_2_dataver1_test')\n",
    "evaluate_model(h97_1__0_2, valid_loader_filtered)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
